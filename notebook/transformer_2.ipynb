{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* trans_run_id = 'transformer_020_8_20210106171509'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from torch.autograd import detect_anomaly\n",
    "import time\n",
    "from fastprogress import master_bar, progress_bar\n",
    "import datetime\n",
    "from sklearn import metrics\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append('../src')\n",
    "from utils import (DataHandler, Timer, seed_everything)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    return metrics.roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../input'\n",
    "FOLD_DIR = '../folds'\n",
    "VALID_SEQ_DIR = '../../kaggle-riiid/data/seq10'\n",
    "SAVE_DIR = '../save'\n",
    "\n",
    "FOLD_NAME = 'vlatest_ALL_2p5M'\n",
    "RANDOM_STATE = 20201209\n",
    "\n",
    "DTYPE = {\n",
    "    'row_id': 'int64',\n",
    "    'timestamp': 'int64',\n",
    "    'user_id': 'int32',\n",
    "    'content_id': 'int16',\n",
    "    'content_type_id': 'int8',\n",
    "    'task_container_id': 'int16',\n",
    "    'user_answer': 'int8',\n",
    "    'answered_correctly': 'int8',\n",
    "    'prior_question_elapsed_time': 'float32',\n",
    "    'prior_question_had_explanation': 'boolean'\n",
    "}\n",
    "\n",
    "TARGET_COLS = ['answered_correctly']\n",
    "\n",
    "model_name = 'transformer_020_8'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "run_id = f'{model_name}_{now:%Y%m%d%H%M%S}'\n",
    "EXP_NAME = f'{FOLD_NAME}__Tran'\n",
    "\n",
    "dh = DataHandler()\n",
    "cfg = dh.load('../exp/000_tran/compe.yml')\n",
    "cfg.update(dh.load(f'../exp/000_tran/{model_name}.yml'))\n",
    "\n",
    "seed_everything(cfg.common.seed)\n",
    "STEP_LENGTH = cfg.data.train.params.step_len\n",
    "\n",
    "cfg.model.params['total_cnt'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compe': {'name': 'riiid-test-answer-prediction'},\n",
       " 'common': {'seed': 2020,\n",
       "  'metrics': {'name': 'auc', 'params': {}},\n",
       "  'drop': ['lecture_idx'],\n",
       "  'debug': False,\n",
       "  'kaggle': {'data': False, 'notebook': False}},\n",
       " 'model': {'backbone': 'transformer_saint_v6_2',\n",
       "  'n_classes': 1,\n",
       "  'epochs': 30,\n",
       "  'params': {'dim_model': 256,\n",
       "   'num_en': 2,\n",
       "   'num_de': 2,\n",
       "   'heads_en': 8,\n",
       "   'heads_de': 8,\n",
       "   'total_ex': 13523,\n",
       "   'total_cat': 7,\n",
       "   'total_tg': 188,\n",
       "   'total_in': 2,\n",
       "   'total_cnt': 2,\n",
       "   'seq_len': 121},\n",
       "  'multi_gpu': True,\n",
       "  'head': None},\n",
       " 'data': {'train': {'dataset_type': 'CustomTrainDataset7_2',\n",
       "   'is_train': True,\n",
       "   'params': {'n_skill': 13523,\n",
       "    'max_seq': 121,\n",
       "    'total_cat': 7,\n",
       "    'total_tg': 188,\n",
       "    'step_len': 150,\n",
       "    'seq_randomness': 0.1},\n",
       "   'loader': {'shuffle': True, 'batch_size': 512, 'num_workers': 4},\n",
       "   'transforms': None},\n",
       "  'valid': {'dataset_type': 'CustomTestDataset7_2',\n",
       "   'is_train': False,\n",
       "   'params': {'n_skill': 13523,\n",
       "    'max_seq': 121,\n",
       "    'total_cat': 7,\n",
       "    'total_tg': 188},\n",
       "   'loader': {'shuffle': False, 'batch_size': 512, 'num_workers': 4},\n",
       "   'transforms': None},\n",
       "  'test': {'dataset_type': 'CustomTestDataset7_2',\n",
       "   'is_train': False,\n",
       "   'params': {'n_skill': 13523, 'max_seq': 121},\n",
       "   'loader': {'shuffle': False, 'batch_size': 512, 'num_workers': 4},\n",
       "   'transforms': None}},\n",
       " 'loss': {'name': 'BCEWithLogitsLoss', 'params': {}},\n",
       " 'optimizer': {'name': 'Adam', 'params': {'lr': 0.0005}},\n",
       " 'scheduler': {'name': 'CosineAnnealingLR', 'params': {'T_max': 30}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "    train_df = pd.read_csv(f'{INPUT_DIR}/train.csv', dtype=DTYPE, nrows=10**6)\n",
    "else:\n",
    "    train_df = pd.read_csv(f'{INPUT_DIR}/train.csv', dtype=DTYPE)\n",
    "    \n",
    "###\n",
    "te_content = pd.read_feather('../input/dl/te_content_id_by_answered_correctly_train.feather')\n",
    "\n",
    "if debug:\n",
    "    te_content  = te_content.iloc[:10**6]\n",
    "\n",
    "train_df['te_content_id_by_answered_correctly'] = te_content['te_content_id_by_answered_correctly'].values\n",
    "###\n",
    "\n",
    "folds = pd.read_feather(f'{FOLD_DIR}/train_folds_{FOLD_NAME}_v2_{RANDOM_STATE}.feather')\n",
    "valid_idx = folds[folds.val == 1]['index'].values\n",
    "if debug:\n",
    "    valid_idx = valid_idx[np.where(valid_idx < len(train_df))]\n",
    "\n",
    "fold_df = pd.DataFrame(index=range(len(train_df)))\n",
    "fold_df['fold_0'] = 0\n",
    "fold_df.loc[valid_idx, 'fold_0'] += 1\n",
    "\n",
    "drop_idx = train_df[train_df.content_type_id != 0].index\n",
    "train_df = train_df.drop(drop_idx, axis=0).reset_index(drop=True)\n",
    "fold_df = fold_df.drop(drop_idx, axis=0).reset_index(drop=True)\n",
    "\n",
    "def make_content_map_dict():\n",
    "    questions_df = pd.read_csv(f'{INPUT_DIR}/questions.csv')\n",
    "    \n",
    "    q2p = dict(questions_df[['question_id', 'part']].values)\n",
    "    q2p = np.array(list(q2p.values())) \n",
    "\n",
    "    questions_df['tags'] = questions_df['tags'].fillna(0)\n",
    "    questions_df['tag_list'] = questions_df['tags'].apply(lambda tags: [int(tag) for tag in str(tags).split(' ')])\n",
    "    questions_df['tag_list'] = questions_df['tag_list'].apply(lambda x: [0] * (6 - len(x)) + x)\n",
    "    q2tg = dict(questions_df[['question_id', 'tag_list']].values)\n",
    "    q2tg = np.array(list(q2tg.values()))\n",
    "\n",
    "    te_dict = dh.load('../input/dl/te_content_id_by_answered_correctly.pkl')\n",
    "    te_df = pd.DataFrame.from_dict(te_dict).sort_index().iloc[:13523]\n",
    "    q2te = np.mean(te_df.values, axis=1)\n",
    "    \n",
    "    q2ws = np.load('../input/dl/q2ws.npy')\n",
    "\n",
    "    return q2p, q2tg, q2te, q2ws\n",
    "\n",
    "q2p, q2tg, q2te, q2ws = make_content_map_dict()\n",
    "\n",
    "target_df = train_df[TARGET_COLS[0]]\n",
    "n_splits = len(fold_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SAINT v6_2\n",
    "# mod based on CustomTrainDataset9\n",
    "\n",
    "class CustomTrainDataset7_2_(Dataset):\n",
    "    def __init__(self, samples, df, q2p, q2tg, q2te, q2ws, cfg=None):\n",
    "        super(CustomTrainDataset7_2_, self).__init__()\n",
    "        self.max_seq = cfg.params.max_seq\n",
    "        self.n_content = cfg.params.n_skill\n",
    "        self.n_tag = cfg.params.total_tg\n",
    "        self.step_length = STEP_LENGTH\n",
    "        self.seq_randomness = cfg.params.seq_randomness\n",
    "        self.samples = samples\n",
    "        self.q2p = q2p\n",
    "        self.q2tg = q2tg\n",
    "        self.q2te = q2te\n",
    "        self.q2ws = q2ws\n",
    "\n",
    "        user_ids = []\n",
    "        for user_id in samples.index:\n",
    "            q = samples[user_id][0]\n",
    "            if len(q) < 2:\n",
    "                continue\n",
    "            user_ids.append(user_id)\n",
    "        self.user_step_ids = df[df['user_id'].isin(user_ids)]['user_step_id'].unique()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_step_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user_step_id = self.user_step_ids[index]\n",
    "        user_id, step_id = list(map(int, user_step_id.split('__')))\n",
    "        \n",
    "        q_, qa_, qt_, qe_, qte_ = self.samples[user_id]\n",
    "\n",
    "        step_start, step_end = step_id * self.step_length, (step_id + 1) * self.step_length\n",
    "        if step_id > 0 and len(q_[step_start: step_end]) < self.step_length:\n",
    "            step_start = (step_id - 1) * self.step_length\n",
    "\n",
    "        q_ = q_[step_start: step_end]\n",
    "        qa_ = qa_[step_start: step_end]\n",
    "        qt_ = qt_[step_start: step_end]\n",
    "        qe_ = qe_[step_start: step_end]\n",
    "        qte_ = qte_[step_start: step_end]\n",
    "\n",
    "        qt_ = qt_ / 60_000.   # ms -> m\n",
    "        qe_ = qe_ / 1_000.   # ms -> s\n",
    "        seq_len = len(q_)\n",
    "\n",
    "        q = np.zeros(self.max_seq, dtype=int)\n",
    "        qa = np.zeros(self.max_seq, dtype=int)\n",
    "        qt = np.zeros(self.max_seq, dtype=int)\n",
    "        qe = np.zeros(self.max_seq, dtype=int)\n",
    "        qte = np.zeros(self.max_seq, dtype=float)\n",
    "        qtg = np.zeros((self.max_seq - 1, 6), dtype=int) + self.n_tag\n",
    "        if seq_len >= self.max_seq:\n",
    "            start = random.randint(0, (seq_len - self.max_seq))\n",
    "            end = start + self.max_seq\n",
    "            q[:] = q_[start: end]\n",
    "            qa[:] = qa_[start: end]\n",
    "            qt[:] = qt_[start: end]\n",
    "            qe[:] = qe_[start: end]\n",
    "            qte[:] = qte_[start: end]\n",
    "        else:\n",
    "            start = 0\n",
    "            end = random.randint(2, seq_len)\n",
    "            seq_len = end - start\n",
    "            q[-seq_len:] = q_[0: seq_len]\n",
    "            qa[-seq_len:] = qa_[0: seq_len]\n",
    "            qt[-seq_len:] = qt_[0: seq_len]\n",
    "            qe[-seq_len:] = qe_[0: seq_len]\n",
    "            qte[-seq_len:] = qte_[0: seq_len]\n",
    "\n",
    "        target_id = q[1:].copy()\n",
    "        label = qa[1:].copy()\n",
    "        ac = qa[:-1].copy()\n",
    "        ###\n",
    "        te_content_id = qte[1:].copy()\n",
    "        ###\n",
    "        \n",
    "        learn_start_idx = np.where(target_id > 0)[0][0]   # 変更した\n",
    "\n",
    "        part = np.zeros(self.max_seq - 1)\n",
    "        part[learn_start_idx:] = self.q2p[target_id[learn_start_idx:]]   # 変更した\n",
    "\n",
    "        difftime = np.diff(qt.copy())\n",
    "        difftime = np.where(difftime < 0, 300, difftime)\n",
    "        difftime = np.log1p(difftime)\n",
    "\n",
    "        prior_elapsed = qe[1:].copy()\n",
    "        prior_elapsed = np.log1p(prior_elapsed)\n",
    "        prior_elapsed = np.where(np.isnan(prior_elapsed), np.log1p(21), prior_elapsed)\n",
    "\n",
    "        qtg[learn_start_idx:, :] = self.q2tg[target_id[learn_start_idx:]]   # 変更した\n",
    "        \n",
    "        ###\n",
    "        te_content_id = np.where(np.isnan(te_content_id), 0.625164097637492, te_content_id)   # nanmean\n",
    "\n",
    "        avg_u_target = np.zeros(self.max_seq - 1, dtype=float)\n",
    "        ac_latest = ac[learn_start_idx:]\n",
    "        avg_u_target[learn_start_idx:] = ac_latest.cumsum() / (np.arange(len(ac_latest)) + 1)\n",
    "        avg_u_target = np.where(np.isnan(avg_u_target), 0, avg_u_target)\n",
    "        \n",
    "        num_feat = np.vstack([te_content_id, avg_u_target]).T\n",
    "        \n",
    "        cnt = np.zeros(self.max_seq - 1)\n",
    "        unique_content_id = []\n",
    "        for idx in range(learn_start_idx, self.max_seq - 1):\n",
    "            id_ = target_id[idx]\n",
    "            if id_ in unique_content_id:\n",
    "                cnt[idx] = 1\n",
    "            else:\n",
    "                cnt[idx] = 0\n",
    "                unique_content_id.append(id_)\n",
    "                \n",
    "        ###\n",
    "\n",
    "        feat = {\n",
    "            'in_ex': torch.LongTensor(target_id),\n",
    "            'in_dt': torch.FloatTensor(difftime),\n",
    "            'in_el': torch.FloatTensor(prior_elapsed),\n",
    "            'in_tag': torch.LongTensor(qtg),\n",
    "            'in_cat': torch.LongTensor(part),\n",
    "            'in_de': torch.LongTensor(ac),\n",
    "            ###\n",
    "            'num_feat': torch.FloatTensor(num_feat),\n",
    "            'in_cnt':torch.LongTensor(cnt) \n",
    "            ###\n",
    "        }\n",
    "\n",
    "        label = torch.FloatTensor(label)\n",
    "\n",
    "        return feat, label\n",
    "\n",
    "\n",
    "class CustomValidDataset7_2_(Dataset):\n",
    "    def __init__(self, samples, df, q2p, q2tg, q2te, q2ws, cfg=None):\n",
    "        super(CustomValidDataset7_2_, self).__init__()\n",
    "        self.max_seq = cfg.params.max_seq\n",
    "        self.n_skill = cfg.params.n_skill\n",
    "        self.n_tag = cfg.params.total_tg\n",
    "        self.samples = samples\n",
    "        self.df = df\n",
    "        self.q2p = q2p\n",
    "        self.q2tg = q2tg\n",
    "        ###\n",
    "        self.q2te = q2te\n",
    "        self.q2ws = q2ws\n",
    "        ###\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        row_id = row['row_id']\n",
    "\n",
    "        seq_list = dh.load(f'{VALID_SEQ_DIR}/row_{int(row_id)}.pkl')\n",
    "\n",
    "        difftime = np.array(seq_list[1]) / 60_000.   # ms -> m\n",
    "        difftime = np.where(difftime < 0, 300, difftime)\n",
    "        difftime = np.log1p(difftime)\n",
    "\n",
    "        prior_elapsed = np.array(seq_list[2]) / 1_000.\n",
    "        prior_elapsed = np.log1p(prior_elapsed)\n",
    "        prior_elapsed = np.where(np.isnan(prior_elapsed), np.log1p(21), prior_elapsed)\n",
    "\n",
    "        content_id = np.array(seq_list[0])\n",
    "        learn_start_idx = np.where(content_id > 0)[0][0]   # 変更した\n",
    "        \n",
    "        part = np.zeros(self.max_seq - 1)\n",
    "        part[learn_start_idx:] = self.q2p[content_id[learn_start_idx:]]   # 変更した\n",
    "        \n",
    "        target = seq_list[3]\n",
    "\n",
    "        qtg = np.zeros((self.max_seq - 1, 6)) + self.n_tag\n",
    "        qtg[learn_start_idx:, :] = self.q2tg[content_id[learn_start_idx:]]   # 変更した\n",
    "        \n",
    "        ###\n",
    "        avg_u_target = np.zeros(self.max_seq - 1, dtype=float)\n",
    "        ac_latest = np.array(target[learn_start_idx:])\n",
    "        avg_u_target[learn_start_idx:] = ac_latest.cumsum() / (np.arange(len(ac_latest)) + 1)\n",
    "        avg_u_target = np.where(np.isnan(avg_u_target), 0, avg_u_target)\n",
    "        \n",
    "        te_content_id = np.zeros(self.max_seq - 1)\n",
    "        te_content_id[learn_start_idx:] = self.q2te[content_id[learn_start_idx:]]\n",
    "        te_content_id = np.where(np.isnan(te_content_id), 0.625164097637492, te_content_id)   # nanmean\n",
    "        \n",
    "        num_feat = np.vstack([te_content_id, avg_u_target]).T\n",
    "        \n",
    "        cnt = np.zeros(self.max_seq - 1)\n",
    "        unique_content_id = []\n",
    "        for idx in range(learn_start_idx, self.max_seq - 1):\n",
    "            id_ = content_id[idx]\n",
    "            if id_ in unique_content_id:\n",
    "                cnt[idx] = 1\n",
    "            else:\n",
    "                cnt[idx] = 0\n",
    "                unique_content_id.append(id_)\n",
    "        ###\n",
    "        \n",
    "        feat = {\n",
    "            'in_ex': torch.LongTensor(content_id),\n",
    "            'in_dt': torch.FloatTensor(difftime),\n",
    "            'in_el': torch.FloatTensor(prior_elapsed),\n",
    "            'in_tag': torch.LongTensor(qtg),\n",
    "            'in_cat': torch.LongTensor(part),\n",
    "            'in_de': torch.LongTensor(target),\n",
    "            ###\n",
    "            'num_feat': torch.FloatTensor(num_feat),\n",
    "            'in_cnt':torch.LongTensor(cnt) ,\n",
    "            ###\n",
    "        }\n",
    "\n",
    "        if TARGET_COLS[0] in self.df.columns:\n",
    "            label = np.append(target[1:], [row[TARGET_COLS[0]]])\n",
    "            label = torch.FloatTensor(label)\n",
    "            return feat, label\n",
    "        else:\n",
    "            return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on\n",
    "# https://github.com/arshadshk/SAINT-pytorch/blob/main/saint.py\n",
    "\n",
    "class Feed_Forward_block(nn.Module):\n",
    "    \"\"\"\n",
    "    out =  Relu( M_out*w1 + b1) *w2 + b2\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_ff):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features=dim_ff, out_features=dim_ff)\n",
    "        self.layer2 = nn.Linear(in_features=dim_ff, out_features=dim_ff)\n",
    "\n",
    "    def forward(self, ffn_in):\n",
    "        return self.layer2(F.relu(self.layer1(ffn_in)))\n",
    "\n",
    "\n",
    "class Encoder_block(nn.Module):\n",
    "    \"\"\"\n",
    "    M = SkipConct(Multihead(LayerNorm(Qin;Kin;Vin)))\n",
    "    O = SkipConct(FFN(LayerNorm(M)))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_model, heads_en, total_ex, total_cat, total_tg, total_cnt, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len - 1\n",
    "        self.embd_ex = nn.Embedding(total_ex, embedding_dim=dim_model)\n",
    "        self.embd_cat = nn.Embedding(total_cat + 1, embedding_dim=dim_model)\n",
    "        self.embd_tg = nn.Embedding(total_tg + 1, embedding_dim=dim_model)\n",
    "        self.embd_cnt = nn.Embedding(total_cnt, embedding_dim=dim_model)\n",
    "        self.embd_pos = nn.Embedding(seq_len, embedding_dim=dim_model)\n",
    "        self.dt_fc = nn.Linear(1, dim_model, bias=False)\n",
    "#         self.dl_fc = nn.Sequential(\n",
    "#             nn.Linear(1, dim_model, bias=False),\n",
    "#             nn.LayerNorm(dim_model)\n",
    "#         )\n",
    "        # self.task_fc = nn.Linear(1, dim_model, bias=False)\n",
    "        self.cate_proj = nn.Sequential(\n",
    "            nn.Linear(dim_model*5, dim_model),\n",
    "            nn.LayerNorm(dim_model),\n",
    "        )   \n",
    "\n",
    "        self.multi_en = nn.MultiheadAttention(embed_dim=dim_model, num_heads=heads_en)\n",
    "        self.ffn_en = Feed_Forward_block(dim_model)\n",
    "        self.layer_norm1 = nn.LayerNorm(dim_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, in_ex, in_cat, in_tg, in_dt, in_cnt, first_block=True):\n",
    "        device = in_ex.device\n",
    "\n",
    "        if first_block:\n",
    "            in_ex = self.embd_ex(in_ex)\n",
    "            in_cat = self.embd_cat(in_cat)\n",
    "\n",
    "            in_dt = in_dt.unsqueeze(-1)\n",
    "            in_dt = self.dt_fc(in_dt)\n",
    "\n",
    "            in_tg = self.embd_tg(in_tg)\n",
    "            avg_in_tg_embed = in_tg.mean(dim=2)\n",
    "            max_in_tg_embed = in_tg.max(dim=2).values\n",
    "            \n",
    "            in_cnt = self.embd_cnt(in_cnt)\n",
    "\n",
    "            # combining the embedings\n",
    "            # out = in_ex + in_cat + in_dt + (avg_in_tg_embed + max_in_tg_embed) + in_cnt\n",
    "            out = torch.cat([in_ex, in_cat, in_dt, (avg_in_tg_embed + max_in_tg_embed), in_cnt], axis=2)\n",
    "            out = self.cate_proj(out)\n",
    "        else:\n",
    "            out = in_ex\n",
    "\n",
    "        in_pos = get_pos(self.seq_len, device)\n",
    "        in_pos = self.embd_pos(in_pos)\n",
    "        out = out + in_pos\n",
    "\n",
    "        out = out.permute(1, 0, 2)\n",
    "\n",
    "        # Multihead attention\n",
    "        n, _, _ = out.shape\n",
    "        out = self.layer_norm1(out)\n",
    "        skip_out = out\n",
    "        out, attn_wt = self.multi_en(out, out, out,\n",
    "                                     attn_mask=get_mask(seq_len=n, device=device))\n",
    "        out = out + skip_out\n",
    "\n",
    "        # feed forward\n",
    "        out = out.permute(1, 0, 2)\n",
    "        out = self.layer_norm2(out)\n",
    "        skip_out = out\n",
    "        out = self.ffn_en(out)\n",
    "        out = out + skip_out\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder_block(nn.Module):\n",
    "    \"\"\"\n",
    "    M1 = SkipConct(Multihead(LayerNorm(Qin;Kin;Vin)))\n",
    "    M2 = SkipConct(Multihead(LayerNorm(M1;O;O)))\n",
    "    L = SkipConct(FFN(LayerNorm(M2)))\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model, total_in, heads_de, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len - 1\n",
    "        self.embd_in = nn.Embedding(total_in, embedding_dim=dim_model)\n",
    "        self.embd_pos = nn.Embedding(self.seq_len, embedding_dim=dim_model)\n",
    "        self.multi_de1 = nn.MultiheadAttention(embed_dim=dim_model, num_heads=heads_de)\n",
    "        self.multi_de2 = nn.MultiheadAttention(embed_dim=dim_model, num_heads=heads_de)\n",
    "        self.ffn_en = Feed_Forward_block(dim_model)\n",
    "        self.el_fc = nn.Linear(1, dim_model, bias=False)\n",
    "#         self.el_fc = nn.Sequential(\n",
    "#             nn.Linear(1, dim_model, bias=False),\n",
    "#             nn.LayerNorm(dim_model)\n",
    "#         )\n",
    "        self.cate_proj = nn.Sequential(\n",
    "            nn.Linear(dim_model*2, dim_model),\n",
    "            nn.LayerNorm(dim_model),\n",
    "        )   \n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(dim_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(dim_model)\n",
    "        self.layer_norm3 = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, in_in, in_el, en_out, first_block=True):\n",
    "        device = in_in.device\n",
    "\n",
    "        if first_block:\n",
    "            in_in = self.embd_in(in_in)\n",
    "\n",
    "            in_el = in_el.unsqueeze(-1)\n",
    "            in_el = self.el_fc(in_el)\n",
    "\n",
    "            # out = in_in + in_el\n",
    "            out = torch.cat([in_in, in_el], axis=2)\n",
    "            out = self.cate_proj(out)\n",
    "        else:\n",
    "            out = in_in\n",
    "\n",
    "        in_pos = get_pos(self.seq_len, device)\n",
    "        in_pos = self.embd_pos(in_pos)\n",
    "        out = out + in_pos\n",
    "\n",
    "        out = out.permute(1, 0, 2)\n",
    "        n, _, _ = out.shape\n",
    "\n",
    "        out = self.layer_norm1(out)\n",
    "        skip_out = out\n",
    "        out, attn_wt = self.multi_de1(out, out, out,\n",
    "                                      attn_mask=get_mask(seq_len=n, device=device))\n",
    "        out = skip_out + out\n",
    "\n",
    "        en_out = en_out.permute(1, 0, 2)\n",
    "        en_out = self.layer_norm2(en_out)\n",
    "        skip_out = out\n",
    "        out, attn_wt = self.multi_de2(out, en_out, en_out,\n",
    "                                      attn_mask=get_mask(seq_len=n, device=device))\n",
    "        out = out + skip_out\n",
    "\n",
    "        out = out.permute(1, 0, 2)\n",
    "        out = self.layer_norm3(out)\n",
    "        skip_out = out\n",
    "        out = self.ffn_en(out)\n",
    "        out = out + skip_out\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def get_mask(seq_len, device):\n",
    "    mask = torch.from_numpy(np.triu(np.ones((seq_len, seq_len)), k=1).astype(bool)).to(device)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_pos(seq_len, device):\n",
    "    # use sine positional embeddinds\n",
    "    return torch.arange(seq_len, device=device).unsqueeze(0)\n",
    "\n",
    "\n",
    "class SAINT(nn.Module):\n",
    "    def __init__(self, dim_model, num_en, num_de, heads_en, total_ex, total_cat, total_tg, total_in, total_cnt,\n",
    "                      heads_de, seq_len, num_fc_in_dim=2, num_fc_out_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_en = num_en\n",
    "        self.num_de = num_de\n",
    "\n",
    "        self.encoder = get_clones(Encoder_block(dim_model, heads_en, total_ex, total_cat, total_tg, total_cnt, seq_len), num_en)\n",
    "        self.decoder = get_clones(Decoder_block(dim_model, total_in, heads_de, seq_len), num_de)\n",
    "        \n",
    "        self.num_fc = nn.Linear(in_features=num_fc_in_dim, out_features=num_fc_out_dim)\n",
    "        self.out_fc1 = nn.Linear(in_features=dim_model, out_features=num_fc_out_dim)\n",
    "        self.out_fc2 = nn.Linear(in_features=num_fc_out_dim * 2, out_features=1)\n",
    "\n",
    "    def forward(self, feat):\n",
    "        in_ex = feat['in_ex']\n",
    "        in_dt = feat['in_dt']\n",
    "        in_el = feat['in_el']\n",
    "        in_tg = feat['in_tag']\n",
    "        in_cat = feat['in_cat']\n",
    "        in_in = feat['in_de']\n",
    "        ###\n",
    "        num_feat = feat['num_feat']\n",
    "        in_cnt = feat['in_cnt']\n",
    "        ###\n",
    "\n",
    "        first_block = True\n",
    "        for x in range(self.num_en):\n",
    "            if x >= 1:\n",
    "                first_block = False\n",
    "            in_ex = self.encoder[x](in_ex, in_cat, in_tg, in_dt, in_cnt, first_block=first_block)\n",
    "            in_cat = in_ex\n",
    "\n",
    "        first_block = True\n",
    "        for x in range(self.num_de):\n",
    "            if x >= 1:\n",
    "                first_block = False\n",
    "            in_in = self.decoder[x](in_in, in_el, en_out=in_ex, first_block=first_block)\n",
    "\n",
    "        num_feat = self.num_fc(num_feat)\n",
    "        in_in = self.out_fc1(in_in)\n",
    "        in_in = torch.cat([in_in, num_feat], dim=2)\n",
    "        in_in = self.out_fc2(in_in)\n",
    "    \n",
    "        return in_in.squeeze(-1)\n",
    "\n",
    "def replace_fc(model, cfg):\n",
    "    return model\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.base_model = SAINT(**cfg['model']['params'])\n",
    "        self.model = replace_fc(self.base_model, cfg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_epoch(model, train_loader, criterion, optimizer, mb):\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "\n",
    "    for feats, targets in progress_bar(train_loader, parent=mb):\n",
    "        if type(feats) == dict:\n",
    "            for k, v in feats.items():\n",
    "                feats[k] = v.to(device)\n",
    "        else:\n",
    "            feats = feats.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        preds = model(feats)\n",
    "\n",
    "        loss = criterion(preds, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "    del feats, targets; gc.collect()\n",
    "    return model, avg_loss\n",
    "\n",
    "def _val_epoch(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    valid_preds = np.zeros((len(valid_loader.dataset), cfg.model.n_classes))\n",
    "\n",
    "    avg_val_loss = 0.\n",
    "    valid_batch_size = valid_loader.batch_size\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (feats, targets) in enumerate(valid_loader):\n",
    "            if type(feats) == dict:\n",
    "                for k, v in feats.items():\n",
    "                    feats[k] = v.to(device)\n",
    "            else:\n",
    "                feats = feats.to(device)\n",
    "\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(feats)\n",
    "\n",
    "            loss = criterion(preds, targets)\n",
    "\n",
    "            preds = preds[:, -1]\n",
    "            valid_preds[i * valid_batch_size: (i + 1) * valid_batch_size, :] = preds.sigmoid().cpu().detach().numpy().reshape(-1, 1)\n",
    "            avg_val_loss += loss.item() / len(valid_loader)\n",
    "    return valid_preds, avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(cfg, PREV_WEIGHT):\n",
    "    \n",
    "    train_df['step'] = train_df.groupby('user_id').cumcount() // STEP_LENGTH\n",
    "    train_df['user_step_id'] = train_df['user_id'].astype(str) + '__' + train_df['step'].astype(str)\n",
    "\n",
    "    oof = np.zeros((len(train_df), cfg.model.n_classes))\n",
    "    cv = 0\n",
    "    col = 'fold_0'\n",
    "\n",
    "    trn_x, val_x = train_df[fold_df[col] == 0], train_df[fold_df[col] > 0]\n",
    "    val_y = target_df[fold_df[col] > 0].values\n",
    "\n",
    "    usecols = ['user_id', 'content_id', 'timestamp', 'prior_question_elapsed_time',\n",
    "                    'answered_correctly', 'te_content_id_by_answered_correctly']\n",
    "    group = (trn_x[usecols]\n",
    "             .groupby('user_id')\n",
    "             .apply(lambda r: (\n",
    "                 r['content_id'].values,\n",
    "                 r['answered_correctly'].values,\n",
    "                 r['timestamp'].values,\n",
    "                 r['prior_question_elapsed_time'].values,\n",
    "                 r['te_content_id_by_answered_correctly'].values\n",
    "             )))\n",
    "\n",
    "    dataset = CustomTrainDataset7_2_(samples=group, df=trn_x, q2p=q2p, q2tg=q2tg, q2te=q2te, q2ws=q2ws, cfg=cfg.data.train)\n",
    "    train_loader = DataLoader(dataset, **cfg.data.train.loader)\n",
    "\n",
    "    dataset = CustomValidDataset7_2_(samples=group, df=val_x, q2p=q2p, q2tg=q2tg, q2te=q2te, q2ws=q2ws, cfg=cfg.data.valid)\n",
    "    valid_loader = DataLoader(dataset, **cfg.data.valid.loader)\n",
    "\n",
    "    is_train = True\n",
    "    model = CustomModel(cfg)\n",
    "    if PREV_WEIGHT != None:\n",
    "        model.load_state_dict(torch.load(PREV_WEIGHT))\n",
    "    model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "\n",
    "    loss_func = getattr(nn, cfg.loss.name)(**cfg.loss.params)\n",
    "    metric_func = auc\n",
    "    optimizer = getattr(torch.optim, cfg.optimizer.name)(params=model.parameters(), **cfg.optimizer.params)\n",
    "    scheduler = getattr(torch.optim.lr_scheduler, cfg.scheduler.name)(\n",
    "        optimizer,\n",
    "        **cfg.scheduler.params,\n",
    "    )\n",
    "\n",
    "    best_epoch = -1\n",
    "    best_val_score = -np.inf\n",
    "    mb = master_bar(range(cfg.model.epochs))\n",
    "\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    val_score_list = []\n",
    "\n",
    "    for epoch in mb:\n",
    "        start_time = time.time()\n",
    "\n",
    "        with detect_anomaly():\n",
    "            model, avg_loss = _train_epoch(model, train_loader, loss_func, optimizer, mb)\n",
    "\n",
    "        valid_preds, avg_val_loss = _val_epoch(model, valid_loader, loss_func)\n",
    "\n",
    "        val_score = metric_func(val_y, valid_preds)\n",
    "\n",
    "        train_loss_list.append(avg_loss)\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "        val_score_list.append(val_score)\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        mb.write(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.6f}  avg_val_loss: {avg_val_loss:.6f} val_score: {val_score:.6f} time: {elapsed:.0f}s')\n",
    "\n",
    "        if val_score > best_val_score:\n",
    "            best_epoch = epoch + 1\n",
    "            best_val_score = val_score\n",
    "            best_valid_preds = valid_preds\n",
    "            if cfg.model.multi_gpu:\n",
    "                best_model = model.module.state_dict()\n",
    "            else:\n",
    "                best_model = model.state_dict()\n",
    "\n",
    "    oof[val_x.index, :] = best_valid_preds\n",
    "    cv += best_val_score * fold_df[col].max()\n",
    "    \n",
    "    torch.save(best_model, f'./weight_best.pt')\n",
    "\n",
    "    print(f'\\nEpoch {best_epoch} - val_score: {best_val_score:.6f}')\n",
    "\n",
    "    print('\\n\\n===================================\\n')\n",
    "    print(f'CV: {cv:.6f}')\n",
    "    print('\\n===================================\\n\\n')\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.409117  avg_val_loss: 0.416337 val_score: 0.779394 time: 1244s\n",
      "Epoch 2 - avg_train_loss: 0.398350  avg_val_loss: 0.412747 val_score: 0.783903 time: 1270s\n",
      "Epoch 3 - avg_train_loss: 0.396012  avg_val_loss: 0.410681 val_score: 0.786210 time: 1266s\n",
      "Epoch 4 - avg_train_loss: 0.394609  avg_val_loss: 0.409273 val_score: 0.788811 time: 1271s\n",
      "Epoch 5 - avg_train_loss: 0.393459  avg_val_loss: 0.408330 val_score: 0.789616 time: 1336s\n",
      "Epoch 6 - avg_train_loss: 0.392714  avg_val_loss: 0.407990 val_score: 0.791058 time: 1346s\n",
      "Epoch 7 - avg_train_loss: 0.392083  avg_val_loss: 0.407583 val_score: 0.791649 time: 1315s\n",
      "Epoch 8 - avg_train_loss: 0.391414  avg_val_loss: 0.406459 val_score: 0.792340 time: 1354s\n",
      "Epoch 9 - avg_train_loss: 0.390781  avg_val_loss: 0.405960 val_score: 0.793000 time: 1387s\n",
      "Epoch 10 - avg_train_loss: 0.390414  avg_val_loss: 0.405179 val_score: 0.793842 time: 1374s\n",
      "Epoch 11 - avg_train_loss: 0.389886  avg_val_loss: 0.405168 val_score: 0.793961 time: 1362s\n",
      "Epoch 12 - avg_train_loss: 0.389636  avg_val_loss: 0.404832 val_score: 0.794836 time: 1364s\n",
      "Epoch 13 - avg_train_loss: 0.389230  avg_val_loss: 0.405805 val_score: 0.795123 time: 1372s\n",
      "Epoch 14 - avg_train_loss: 0.388919  avg_val_loss: 0.404236 val_score: 0.795277 time: 1362s\n",
      "Epoch 15 - avg_train_loss: 0.388588  avg_val_loss: 0.403984 val_score: 0.795558 time: 1327s\n",
      "Epoch 16 - avg_train_loss: 0.388305  avg_val_loss: 0.403845 val_score: 0.795579 time: 1349s\n",
      "Epoch 17 - avg_train_loss: 0.388050  avg_val_loss: 0.403522 val_score: 0.796451 time: 1325s\n",
      "Epoch 18 - avg_train_loss: 0.387784  avg_val_loss: 0.403369 val_score: 0.796334 time: 1444s\n",
      "Epoch 19 - avg_train_loss: 0.387632  avg_val_loss: 0.403940 val_score: 0.796426 time: 1324s\n",
      "Epoch 20 - avg_train_loss: 0.387407  avg_val_loss: 0.403227 val_score: 0.796906 time: 1452s\n",
      "Epoch 21 - avg_train_loss: 0.387182  avg_val_loss: 0.403112 val_score: 0.796699 time: 1292s\n",
      "Epoch 22 - avg_train_loss: 0.387014  avg_val_loss: 0.403043 val_score: 0.796910 time: 1377s\n",
      "Epoch 23 - avg_train_loss: 0.386753  avg_val_loss: 0.402814 val_score: 0.797189 time: 1334s\n",
      "Epoch 24 - avg_train_loss: 0.386657  avg_val_loss: 0.402859 val_score: 0.797242 time: 1292s\n",
      "Epoch 25 - avg_train_loss: 0.386399  avg_val_loss: 0.402693 val_score: 0.797213 time: 1310s\n",
      "Epoch 26 - avg_train_loss: 0.386133  avg_val_loss: 0.402761 val_score: 0.797481 time: 1447s\n",
      "Epoch 27 - avg_train_loss: 0.386117  avg_val_loss: 0.402605 val_score: 0.797461 time: 1283s\n",
      "Epoch 28 - avg_train_loss: 0.385849  avg_val_loss: 0.402562 val_score: 0.797353 time: 1831s\n",
      "Epoch 29 - avg_train_loss: 0.385665  avg_val_loss: 0.402811 val_score: 0.797483 time: 1540s\n",
      "Epoch 30 - avg_train_loss: 0.385450  avg_val_loss: 0.402583 val_score: 0.797657 time: 1461s\n",
      "\n",
      "Epoch 30 - val_score: 0.797657\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "CV: 0.797657\n",
      "\n",
      "===================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = exp(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riiid_torch",
   "language": "python",
   "name": "riiid_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
